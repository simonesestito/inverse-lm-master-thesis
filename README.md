# Inverse Language Modeling for Robust and Grounded LLMs

**Master's Degree Thesis in Computer Science**

To further understand adversarial robustness in LLMs, we propose **Inverse Language Modeling (ILM)**, a unified framework that simultaneously:
1) improves the robustness of LLMs to input perturbations, and, at the same time,
2) enables native grounding by inverting model outputs to identify potentially toxic or unsafe input triggers.

ILM transforms LLMs from static generators into analyzable and robust systems.
ILM can lay the foundation for next-generation LLMs that are not only robust and grounded but also fundamentally more controllable and trustworthy.

**Keywords:** Artificial Intelligence Safety, Large Language Models, Robustness, Adversarial Attacks, Inverse Problems

<div>
    <a href="https://raw.githubusercontent.com/simonesestito/inverse-lm-master-thesis/main/thesis.pdf">
        <img src=".github/assets/arxiv-button.svg" height="80">
    </a>
    <a href="https://raw.githubusercontent.com/simonesestito/inverse-lm-master-thesis/main/thesis.pdf">
        <img src=".github/assets/thesis-button.svg" height="80">
    </a>
    <a href="https://raw.githubusercontent.com/simonesestito/inverse-lm-master-thesis/main/presentation.pdf">
        <img src=".github/assets/presentation-button.svg" height="80">
    </a>
</div>

<div>
    <a href="https://omnai.di.uniroma1.it/">
        <img src=".github/assets/omnai.png" height="70">
    </a>
    <img src=".github/assets/sapienza.png" height="80">
    <img src=".github/assets/nextgeneu-logo.png" height="60">
</div>


---

This repository contains:
- the LaTeX source code of the Master Thesis
- the LaTeX source code of the ArXiv preprint

The source code for the experiment is available at:
https://github.com/davegabe/pag-llm

For authenticity verification, all commits have been either done via the GitHub web UI or signed with my own [GPG key](https://github.com/simonesestito.gpg).